{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qyIro0BslCl"
   },
   "source": [
    "### **–ë–ò–ë–õ–ò–û–¢–ï–ö–ò**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_LrpcDdJHke",
    "outputId": "b4f2907a-a908-4dd7-db79-bf4801c81b9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# –∑–∞–≥—Ä—É–∑–∏–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "\n",
    "# !pip install python-telegram-bot==13.8 --quiet\n",
    "# !pip install python-telegram-bot --upgrade --quiet\n",
    "# !pip install pymorphy2[fast] annoy stop_words transformers sentencepiece sentence_transformers faiss --quiet\n",
    "# !apt install libomp-dev\n",
    "\n",
    "from telegram import Update\n",
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import faiss\n",
    "# !cat '/content/drive/MyDrive/Colab Notebooks/NLP/CHAT_BOT/tokenization_small100.py'\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/NLP/CHAT_BOT')\n",
    "import tokenization_small100\n",
    "from tokenization_small100 import SMALL100Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, TFAutoModel, AutoModelForCausalLM, AutoModel, M2M100ForConditionalGeneration, MarianTokenizer, MarianMTModel, AutoModelForSequenceClassification\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_O4r24ssxAS"
   },
   "source": [
    "### **–î–ê–ù–ù–´–ï, –ú–û–î–ï–õ–ò, –ü–ê–†–ê–ú–ï–¢–†–´**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J9HXxhsItTCb"
   },
   "outputs": [],
   "source": [
    "PATH_TOKEN = '/content/token.txt'\n",
    "PATH_DF_FILMS = '/content/drive/MyDrive/Colab Notebooks/NLP/CHAT_BOT/asian_dramas_dataset_FINAL.csv'\n",
    "PATH_EMB_FILMS = '/content/drive/MyDrive/Colab Notebooks/NLP/CHAT_BOT/df_films_emb.npy'\n",
    "PATH_BERT_CLASS = '/content/drive/MyDrive/Colab Notebooks/NLP/CHAT_BOT/model_bert_clean.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-IcUaD4Tk9o"
   },
   "outputs": [],
   "source": [
    "# —Ç–æ–∫–µ–Ω –¥–ª—è –¢–µ–ª–µ–≥—Ä–∞–º–∞\n",
    "with open(PATH_TOKEN, 'r') as f:\n",
    "  TOKEN = f.read()\n",
    "\n",
    "# –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words('ru') + nltk.corpus.stopwords.words('russian'))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏–Ω—Ç–µ–Ω—Ç–æ–≤\n",
    "history = {'text': [], 'label': []}\n",
    "history_for_gpt = []\n",
    "\n",
    "# –¥–∞–Ω–Ω—ã–µ —Å —Ñ–∏–ª—å–º–∞–º–∏\n",
    "df_films = pd.read_csv(PATH_DF_FILMS).drop('Unnamed: 0', axis=1)\n",
    "embeddings_for_films = np.load(PATH_EMB_FILMS)\n",
    "\n",
    "# —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è BERT-–º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–Ω—Ç–µ–Ω—Ç–æ–≤\n",
    "model_name_for_classification = 'sberbank-ai/ruBert-large'\n",
    "tokenizer_for_classification = AutoTokenizer.from_pretrained(model_name_for_classification)\n",
    "model_for_classification = AutoModelForSequenceClassification.from_pretrained(model_name_for_classification, num_labels=3)\n",
    "device_cpu = torch.device('cpu')\n",
    "model_for_classification.load_state_dict(torch.load(PATH_BERT_CLASS, map_location=device_cpu))\n",
    "\n",
    "# GPT-–º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (–∏–Ω—Ç–µ–Ω—Ç-1)\n",
    "model_name_for_textgeneration = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
    "tokenizer_for_textgeneration = AutoTokenizer.from_pretrained(model_name_for_textgeneration)\n",
    "model_for_textgeneration = AutoModelForCausalLM.from_pretrained(model_name_for_textgeneration)\n",
    "\n",
    "# –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–±–µ–¥–∏–Ω–≥–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–∏—Å–∫–∞ —Ñ–∏–ª—å–º–æ–≤ (–∏–Ω—Ç–µ–Ω—Ç-2)\n",
    "encoder_for_films = SentenceTransformer('sberbank-ai/ruBert-base')\n",
    "\n",
    "# –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∞–Ω—Ç–∏–º–µ–Ω—Ç–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Ñ–∏–ª—å–º–æ–≤\n",
    "model_name_for_santiment = 'Tatyana/rubert-base-cased-sentiment-new'\n",
    "tokenizer_for_santiment = AutoTokenizer.from_pretrained(model_name_for_santiment)\n",
    "model_for_santiment = AutoModelForSequenceClassification.from_pretrained(model_name_for_santiment)\n",
    "\n",
    "# –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∫–æ—Ä–µ–π—Å–∫–∏–π —è–∑—ã–∫ (–∏–Ω—Ç–µ–Ω—Ç-3)\n",
    "model_name_for_translation = 'alirezamsh/small100'\n",
    "model_for_translation = M2M100ForConditionalGeneration.from_pretrained(model_name_for_translation)\n",
    "tokenizer_for_translation = SMALL100Tokenizer.from_pretrained(model_name_for_translation)\n",
    "tokenizer_for_translation.tgt_lang = 'ko'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_OHRNE1zpRY"
   },
   "source": [
    "### **–ú–ï–¢–û–î–´, –§–£–ù–ö–¶–ò–ò**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "KwSddwhBJbFn"
   },
   "outputs": [],
   "source": [
    "# –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "def preprocess_txt(line):\n",
    "    spls = ''.join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != '']\n",
    "    return ' '.join(spls)\n",
    "\n",
    "\n",
    "# –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–Ω—Ç–µ–Ω—Ç–æ–≤\n",
    "def classifier_intents(text):\n",
    "  inputs = tokenizer_for_classification(text, return_tensors='pt')\n",
    "  outputs = model_for_classification(**inputs)\n",
    "  prediction_logits = outputs.logits\n",
    "  prediction = np.argmax(prediction_logits.detach().numpy())\n",
    "  return prediction, prediction_logits\n",
    "\n",
    "\n",
    "# –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã '/start'\n",
    "def start(update: Update, context: CallbackContext):\n",
    "    update.message.reply_text(\n",
    "        '–ü—Ä–∏–≤–µ—Ç! \\\n",
    "        \\n–Ø *AsiaLove –±–æ—Ç*. –Ø —É–º–µ—é –±–æ–ª—Ç–∞—Ç—å, —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å —Å–µ—Ä–∏–∞–ª—ã –∏ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å \\\n",
    "        —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã –Ω–∞ –∫–æ—Ä–µ–π—Å–∫–∏–π —è–∑—ã–∫ üòä \\\n",
    "        \\n–ï—Å–ª–∏ —è —á—Ç–æ-—Ç–æ –Ω–µ –ø–æ–Ω—è–ª, –Ω–µ –∑–ª–∏—Å—å, –ø–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å \\\n",
    "        –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ ^-^.  –ü–æ–≥–Ω–∞–ª–∏!\\n‚ÅâÔ∏è –ö—Å—Ç–∞—Ç–∏, —á—Ç–æ–±—ã —è —Å–º–æ–≥ —Å–¥–µ–ª–∞—Ç—å –ø–µ—Ä–µ–≤–æ–¥ \\\n",
    "        –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –ø–æ—Å—Ç–∞–≤—å —Å–ª–æ–≤–æ/—Ñ—Ä–∞–∑—É –≤ –∑–Ω–∞–∫–∏ \\\n",
    "        —Ä–∞–≤–Ω–æ --> –ü–µ—Ä–µ–≤–µ–¥–∏ –º–Ω–µ —Å–ª–æ–≤–æ =–ê–ø–µ–ª—å—Å–∏–Ω=.', parse_mode='Markdown')\n",
    "\n",
    "\n",
    "# –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ (–∏–Ω—Ç–µ–Ω—Ç-1)\n",
    "def respond_to_dialog(texts):\n",
    "    prefix = '\\n–í–æ–ø—Ä–æ—Å:'\n",
    "    for i, t in enumerate(texts):\n",
    "        prefix += t\n",
    "        prefix += '\\n–í–æ–ø—Ä–æ—Å:' if i % 2 == 1 else '\\n–û—Ç–≤–µ—Ç:'\n",
    "    tokens = tokenizer_for_textgeneration(prefix, return_tensors='pt')\n",
    "    tokens = {k: v for k, v in tokens.items()}\n",
    "    end_token_id = tokenizer_for_textgeneration.encode('\\n')[0]\n",
    "    size = tokens['input_ids'].shape[1]\n",
    "    output = model_for_textgeneration.generate(\n",
    "        **tokens, \n",
    "        eos_token_id=end_token_id,\n",
    "        do_sample=True, \n",
    "        max_new_tokens=size+40, \n",
    "        repetition_penalty=3.2, \n",
    "        temperature=0.5,\n",
    "        num_beams=5,\n",
    "        length_penalty=0.05,\n",
    "        pad_token_id=tokenizer_for_textgeneration.eos_token_id,\n",
    "        min_length=10)\n",
    "    decoded = tokenizer_for_textgeneration.decode(output[0])\n",
    "    result = decoded[len(prefix):]\n",
    "    return result.strip()\n",
    "\n",
    "\n",
    "# —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ñ–∏–ª—å–º–æ–≤ (–∏–Ω—Ç–µ–Ω—Ç-2)\n",
    "def get_films_recom(user_text):\n",
    "  dimension = embeddings_for_films.shape[1]\n",
    "  k_nears = 10\n",
    "  text = encoder_for_films.encode(user_text).reshape(1, -1)\n",
    "  index = faiss.IndexFlatL2(dimension)\n",
    "  index.add(embeddings_for_films)\n",
    "  D, I = index.search(text, k_nears)\n",
    "  dict_temp = {I[0][i]:D[0][i] for i in range(len(D[0]))}\n",
    "  dict_temp = dict(sorted(dict_temp.items(), key=lambda item: item[1], reverse=True))\n",
    "  return list(dict_temp.keys())\n",
    "\n",
    "\n",
    "# –ø–∞—Ä—Å–∏–Ω–≥ —Ä–µ–π—Ç–∏–Ω–≥–∞ —Ñ–∏–ª—å–º–∞/—Å–µ—Ä–∏–∞–ª–∞ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞\n",
    "def get_film_rating(url):\n",
    "  try:\n",
    "    request = requests.get(url)\n",
    "    rating = bs(request.text, 'html.parser').find_all('div', class_='unit-rating')\n",
    "    return rating[0].span.text\n",
    "  except:\n",
    "    return '-'\n",
    "\n",
    "\n",
    "# –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–µ—Ç–∫–∏ —Å–∞–Ω—Ç–∏–º–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Ñ–∏–ª—å–º–æ–≤\n",
    "def predict(text):\n",
    "    inputs = tokenizer_for_santiment(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "      outputs = model_for_santiment(**inputs)\n",
    "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    predicted = torch.argmax(predicted, dim=1).numpy()[0]\n",
    "    return predicted\n",
    "\n",
    "\n",
    "# –ø–æ–¥—Å—á–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –ø–æ —Å–∞–Ω—Ç–∏–º–µ–Ω—Ç—É\n",
    "def count_comments_santiment(url):\n",
    "\n",
    "  list_comments = []\n",
    "  neutral = 0\n",
    "  positive = 0\n",
    "  negative = 0\n",
    "\n",
    "  try:\n",
    "    request = requests.get(url)\n",
    "    comments = bs(request.text, 'html.parser').find_all('div', class_='ct-text clearfix')    \n",
    "    for item in comments:\n",
    "      list_comments.append(item.text.strip())\n",
    "    for cmnt in list_comments:\n",
    "      santiment_label = predict(cmnt)\n",
    "      if santiment_label == 0:\n",
    "        neutral += 1\n",
    "      elif santiment_label == 1:\n",
    "        positive += 1\n",
    "      elif santiment_label == 2:\n",
    "        negative += 1\n",
    "    return '*–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ*: ' + str(neutral) + '\\n*–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ*: '+ str(positive) + '\\n*–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ*: ' + str(negative)\n",
    "  except:\n",
    "    return '*–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ* -' + '\\n*–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ*: -'+ '\\n*–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ*: -'\n",
    "\n",
    "\n",
    "# –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∫–æ—Ä–µ–π—Å–∫–∏–π —è–∑—ã–∫ (–∏–Ω—Ç–µ–Ω—Ç-3)\n",
    "def translate_to_korean(text):\n",
    "  text_to_translate = re.findall(r'=(.*?)=', text)  \n",
    "  tokens = tokenizer_for_translation(text_to_translate, return_tensors='pt')\n",
    "  outputs = model_for_translation.generate(**tokens)\n",
    "  translation = tokenizer_for_translation.batch_decode(outputs, skip_special_tokens=True)\n",
    "  return translation[0]\n",
    "\n",
    "\n",
    "# –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —Ç—Ä–µ–º—è –∏–Ω—Ç–µ–Ω—Ç–∞–º–∏\n",
    "def bot_answers(update: Update, context: CallbackContext):   \n",
    "    user_text = update.message.text \n",
    "    user_text_clean = preprocess_txt(user_text)    \n",
    "    label_pred, stats = classifier_intents(user_text_clean)    \n",
    "\n",
    "    # —Ä–µ–∂–∏–º —Ä–∞–∑–≥–æ–≤–æ—Ä–∞      \n",
    "    if label_pred == 0:\n",
    "      # update.message.reply_text('Talk')\n",
    "      history_for_gpt.append(user_text)\n",
    "      result = respond_to_dialog(history_for_gpt)\n",
    "      history_for_gpt.append(result)   \n",
    "      update.message.reply_text(result)\n",
    "\n",
    "      # –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "      if len(history_for_gpt) > 1000:\n",
    "        with open('/content/drive/MyDrive/Colab Notebooks/NLP/CHAT_BOT/history_for_gpt_'+datetime.now().strftime(\"%d%m%Y_%H%M\")+'.txt', 'w') as f:\n",
    "          f.write(('\\n').join(history_for_gpt))    \n",
    "\n",
    "    # —Ä–µ–∂–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Ñ–∏–ª—å–º–æ–≤\n",
    "    elif label_pred == 1:\n",
    "      # update.message.reply_text('Recommendation')\n",
    "      ids_recom = get_films_recom(user_text_clean)   \n",
    "      for idx in ids_recom[:3]:\n",
    "        # try:\n",
    "        #   update.message.reply_photo(df_films.img_link.iloc[idx])\n",
    "        # except:\n",
    "        #   update.message.reply_text('–£–ø—Å... –ö–∞—Ä—Ç–∏–Ω–∫—É –Ω–µ –≥—Ä—É–∑–∏—Ç—Å—è.\\n')\n",
    "        update.message.reply_text('\\n'.join(['*–ù–∞–∑–≤–∞–Ω–∏–µ*: '+ df_films.ru_name.iloc[idx], \n",
    "                                            '*–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ*: '+ df_films.orig_name.iloc[idx],\n",
    "                                            '*–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ—Ä–∏–π*: '+ df_films.number_ep.iloc[idx],\n",
    "                                            '*–ñ–∞–Ω—Ä*: '+ df_films.genre.iloc[idx],                             \n",
    "                                            '*–°—Ç—Ä–∞–Ω–∞*: '+ df_films.country.iloc[idx], \n",
    "                                            '*–ì–æ–¥*: '+ df_films.year.iloc[idx], \n",
    "                                            '*–†–µ–π—Ç–∏–Ω–≥*: '+ get_film_rating(df_films.page_link.iloc[idx]),\n",
    "                                            '*–°—Å—ã–ª–∫–∞*: '+ df_films.page_link.iloc[idx]]), parse_mode='Markdown')\n",
    "        update.message.reply_text('–ß–∏—Ç–∞—é –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏...\\n')\n",
    "        update.message.reply_text(count_comments_santiment(df_films.page_link.iloc[idx]), parse_mode='Markdown')\n",
    "    \n",
    "    \n",
    "    # —Ä–µ–∂–∏–º –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∫–æ—Ä–µ–π—Å–∫–∏–π —è–∑—ã–∫\n",
    "    elif label_pred == 2:\n",
    "      # update.message.reply_text('Translation')\n",
    "      update.message.reply_text(translate_to_korean(user_text))    \n",
    "\n",
    "    # –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "    history['text'].append(user_text)\n",
    "    history['label'].append(label_pred)\n",
    "    if len(history['label']) > 1000:      \n",
    "      with open('/content/drive/MyDrive/Colab Notebooks/NLP/CHAT_BOT/history_'+datetime.now().strftime(\"%d%m%Y_%H%M\")+'.txt', 'w') as f:\n",
    "        f.write(json.dumps(history, default=str))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSGY_WM-znVB"
   },
   "source": [
    "### **–ó–ê–ü–£–°–ö –ë–û–¢–ê**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "FnTDXmiBKhBV"
   },
   "outputs": [],
   "source": [
    "updater = Updater(TOKEN, use_context=True)\n",
    "dispatcher = updater.dispatcher\n",
    "\n",
    "# –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã '/start'\n",
    "dispatcher.add_handler(CommandHandler('start', start))\n",
    "\n",
    "# –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, bot_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uMJb8JK2KhEN"
   },
   "outputs": [],
   "source": [
    "# –∑–∞–ø—É—Å–∫ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π\n",
    "updater.start_polling()\n",
    "updater.idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiNA--XWJbKx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
